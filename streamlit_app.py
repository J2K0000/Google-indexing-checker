import streamlit as st
import requests
from bs4 import BeautifulSoup
import time
import pandas as pd

# --- Configuration de la page Streamlit ---
st.set_page_config(
    page_title="V√©rificateur d'Indexation Google",
    page_icon="üîé",
    layout="centered"
)

# --- Logique de v√©rification (Corrig√©e v6) ---
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7'
}

def _execute_search(query: str, url: str) -> str:
    """
    Fonction d'aide : Ex√©cute UNE requ√™te Google et v√©rifie la pr√©sence de l'URL.
    Utilise la logique v5 (recoller les <cite>) qui est la plus fiable.
    Retourne "Indexed", "Not Found", ou "Blocked".
    """
    google_search_url = f"https://www.google.com/search?q={query}&hl=fr&cr=countryFR"
    
    try:
        response = requests.get(google_search_url, headers=HEADERS, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        page_text = soup.get_text()

        # 1. V√©rification du blocage ou CAPTCHA
        if "CAPTCHA" in response.text or "nos syst√®mes ont d√©tect√© un trafic inhabituel" in page_text:
            return "Blocked"
        
        # 2. V√©rification des phrases claires de "non-indexation"
        if "Aucun document ne correspond" in page_text or "Il se peut qu'aucun bon r√©sultat ne corresponde" in page_text:
            return "Not Found"

        # 3. V√©rification de la preuve (logique v5)
        cite_tags = soup.find_all('cite')
        url_with_protocol = url.rstrip('/') 
        url_without_protocol = url_with_protocol.replace("https://", "").replace("http://", "")

        for cite in cite_tags:
            # Correction v5 : recolle les morceaux de texte dans les <cite>
            cite_text = cite.get_text(separator="")
            
            if url_with_protocol in cite_text or url_without_protocol in cite_text:
                return "Indexed" # TROUV√â !
        
        # 4. Si non bloqu√©, pas de "aucun doc", et pas dans les <cite> -> non trouv√©
        return "Not Found"

    except requests.exceptions.HTTPError:
        return "Blocked" # Erreur 429, 503, etc.
    except requests.exceptions.RequestException:
        return "Blocked" # Erreur de connexion
    except Exception:
        return "Blocked" # Erreur inattendue
    finally:
        # Pause obligatoire pour ne pas se faire bloquer
        time.sleep(1.0)

def check_google_indexing(url: str) -> dict:
    """
    V√©rifie si une URL est index√©e sur Google en suivant la logique
    multi-√©tapes fournie par l'utilisateur.
    Version 6 : Impl√©mente √âtape 1 (site:) PUIS √âtape 2 (URL exacte).
    """
    result = {"URL": url, "Statut": ""}

    # --- √âTAPE 1 : Recherche avec 'site:' ---
    status1 = _execute_search(query=f"site:{url}", url=url)
    
    if status1 == "Indexed":
        result["Statut"] = "‚úÖ Index√©e (via 'site:')"
        return result
    
    if status1 == "Blocked":
        result["Statut"] = "üö´ CAPTCHA/Blocage (√âtape 1)"
        return result

    # --- √âTAPE 2 : Recherche avec URL exacte (si √âtape 1 a √©chou√©) ---
    # (status1 doit √™tre "Not Found" pour arriver ici)
    
    # On cherche l'URL exacte, entre guillemets
    status2 = _execute_search(query=f'"{url}"', url=url)

    if status2 == "Indexed":
        result["Statut"] = "‚úÖ Index√©e (via URL exacte)"
        return result
    
    if status2 == "Blocked":
        result["Statut"] = "üö´ CAPTCHA/Blocage (√âtape 2)"
        return result

    # --- √âTAPE 3 : Non trouv√©e apr√®s les 2 √©tapes ---
    result["Statut"] = "‚ùå Non Index√©e"
    return result

# --- Interface de l'application Streamlit (inchang√©e) ---

st.title("üîé V√©rificateur d'Indexation Google (Version Corrig√©e v6)")
st.write("Collez une ou plusieurs URLs (une par ligne) pour v√©rifier si elles sont *r√©ellement* index√©es par Google.")
st.write("Cette version utilise une v√©rification en 2 √©tapes (`site:URL` puis `\"URL\"`) pour plus de fiabilit√©.")

# Zone de texte pour les URLs
urls_text = st.text_area("Liste d'URLs √† v√©rifier", height=200, placeholder="https://www.example.com/page1\nhttps://www.example.com/page2")

# Bouton pour lancer la v√©rification
if st.button("üöÄ Lancer la v√©rification"):
    urls_to_check = [url.strip() for url in urls_text.splitlines() if url.strip()]
    
    if not urls_to_check:
        st.warning("Veuillez entrer au moins une URL.")
    else:
        # Barre de progression
        progress_bar = st.progress(0)
        results = []
        
        # Affiche un message pendant le traitement
        status_text = st.empty()
        
        for i, url in enumerate(urls_to_check):
            # Chaque URL peut prendre ~2 secondes (1s par √©tape)
            status_text.text(f"V√©rification de {i+1}/{len(urls_to_check)} : {url}")
            results.append(check_google_indexing(url))
            # Met √† jour la barre de progression
            progress_bar.progress((i + 1) / len(urls_to_check))
        
        status_text.success("V√©rification termin√©e !")
        
        # Affiche les r√©sultats dans un tableau propre
        df = pd.DataFrame(results)
        st.dataframe(df, use_container_width=True)

        # Ajoute une note d'avertissement sur la fiabilit√©
        st.info(
            "**Note :** Cet outil utilise le 'scraping' de Google. "
            "Si vous voyez beaucoup d'erreurs 'üö´ CAPTCHA/Blocage', "
            "cela signifie que Google a temporairement bloqu√© votre adresse IP. "
            "R√©essayez plus tard."
        )

