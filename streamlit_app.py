import streamlit as st
import requests
from bs4 import BeautifulSoup
import time
import pandas as pd

# --- Configuration de la page Streamlit ---
st.set_page_config(
    page_title="V√©rificateur d'Indexation Google",
    page_icon="üîé",
    layout="centered"
)

# --- Logique de v√©rification (Corrig√©e) ---
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
    'Accept-Language': 'fr-FR,fr;q=0.9,en-US;q=0.8,en;q=0.7'
}

def check_google_indexing(url: str) -> dict:
    """
    V√©rifie si une URL est index√©e sur Google et retourne un dictionnaire avec les d√©tails.
    La logique est plus stricte pour diff√©rencier "Index√©e" de "R√©sultats similaires".
    """
    query = f"site:{url}"
    # On force la langue (hl=fr) et le pays (cr=countryFR) pour des r√©sultats stables
    google_search_url = f"https://www.google.com/search?q={query}&hl=fr&cr=countryFR"
    
    result = {"URL": url, "Statut": ""}

    try:
        response = requests.get(google_search_url, headers=HEADERS, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        page_text = soup.get_text() # Pour les v√©rifications de texte g√©n√©riques

        # --- LOGIQUE DE V√âRIFICATION CORRIG√âE ---

        # 1. V√©rification du blocage ou CAPTCHA (prioritaire)
        # "nos syst√®mes ont d√©tect√© un trafic inhabituel" est le signe de blocage IP
        if "CAPTCHA" in response.text or "nos syst√®mes ont d√©tect√© un trafic inhabituel" in page_text:
            result["Statut"] = "üö´ CAPTCHA/Blocage"
        
        # 2. V√©rification des phrases claires de "non-indexation"
        # C'est le cas o√π Google ne trouve absolument rien.
        elif "Aucun document ne correspond" in page_text:
            result["Statut"] = "‚ùå Non Index√©e"
        
        # C'est le cas o√π Google ne trouve pas l'URL exacte, mais propose des alternatives.
        elif "Il se peut qu'aucun bon r√©sultat ne corresponde" in page_text:
             result["Statut"] = "‚ùå Non Index√©e"

        # 3. Si aucune phrase de "non-indexation" n'est trouv√©e,
        #    cela ne signifie pas que l'URL est index√©e (c'√©tait l'erreur).
        #    Google peut afficher des r√©sultats *similaires* du m√™me domaine.
        #    Nous devons donc chercher la *preuve* que notre URL exacte est pr√©sente.
        #    La balise <cite> est la plus fiable pour √ßa, car elle affiche l'URL du r√©sultat.
        else:
            cite_tags = soup.find_all('cite')
            found_in_cite = False
            for cite in cite_tags:
                # On v√©rifie si l'URL exacte est dans le texte de la balise <cite>
                # (Google peut ajouter '...' ou couper, mais l'URL principale doit y √™tre)
                if url in cite.get_text():
                    found_in_cite = True
                    break
            
            if found_in_cite:
                result["Statut"] = "‚úÖ Index√©e"
            else:
                # Si on est ici, c'est que Google n'a pas dit "aucun r√©sultat",
                # mais notre URL n'est pas non plus dans les balises <cite>.
                # C'est le cas o√π il montre des pages du domaine, mais pas celle-ci.
                # C'est donc "Non Index√©e" pour cette URL sp√©cifique.
                result["Statut"] = "‚ùå Non Index√©e (r√©sultats similaires)"

    except requests.exceptions.HTTPError as http_err:
        # G√®re les erreurs 403, 429 (trop de requ√™tes), 503, etc.
        result["Statut"] = f"üö´ Erreur HTTP : {http_err.response.status_code}"
    except requests.exceptions.RequestException:
        result["Statut"] = "üö´ Erreur de connexion"
    except Exception:
        result["Statut"] = "üö´ Erreur inattendue"
    
    # Petite pause pour ne pas surcharger Google et r√©duire le risque de blocage
    time.sleep(0.5)
    return result

# --- Interface de l'application Streamlit (inchang√©e) ---

st.title("üîé V√©rificateur d'Indexation Google (Version Corrig√©e)")
st.write("Collez une ou plusieurs URLs (une par ligne) pour v√©rifier si elles sont *r√©ellement* index√©es par Google (via la commande site:).")

# Zone de texte pour les URLs
urls_text = st.text_area("Liste d'URLs √† v√©rifier", height=200, placeholder="https://www.example.com/page1\nhttps://www.example.com/page2")

# Bouton pour lancer la v√©rification
if st.button("üöÄ Lancer la v√©rification"):
    urls_to_check = [url.strip() for url in urls_text.splitlines() if url.strip()]
    
    if not urls_to_check:
        st.warning("Veuillez entrer au moins une URL.")
    else:
        # Barre de progression
        progress_bar = st.progress(0)
        results = []
        
        # Affiche un message pendant le traitement
        status_text = st.empty()
        
        for i, url in enumerate(urls_to_check):
            status_text.text(f"V√©rification de {i+1}/{len(urls_to_check)} : {url}")
            results.append(check_google_indexing(url))
            # Met √† jour la barre de progression
            progress_bar.progress((i + 1) / len(urls_to_check))
        
        status_text.success("V√©rification termin√©e !")
        
        # Affiche les r√©sultats dans un tableau propre
        df = pd.DataFrame(results)
        st.dataframe(df, use_container_width=True)

        # Ajoute une note d'avertissement sur la fiabilit√©
        st.info(
            "**Note :** Cet outil utilise le 'scraping' de Google. "
            "Si vous voyez beaucoup d'erreurs 'üö´ CAPTCHA/Blocage', "
            "cela signifie que Google a temporairement bloqu√© votre adresse IP. "
            "R√©essayez plus tard."
        )
